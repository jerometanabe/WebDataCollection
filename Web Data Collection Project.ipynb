{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff88f17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import requests\n",
    "import time\n",
    "import datetime as dt\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data collection for a single product from a URL\n",
    "#Part 1 of the string\n",
    "\n",
    "#Preparing CSV that information will be added to\n",
    "productHeader = ['title', 'genre', 'price', 'quantity', 'in_stock', 'UPC', 'product_type', 'date_accessed']\n",
    "with open('ProductInformation.csv', 'w', newline = '', encoding = 'UTF8') as a:\n",
    "    writer = csv.writer(a)\n",
    "    writer.writerow(productHeader)\n",
    "\n",
    "\n",
    "def productExtract(URL):\n",
    "    #Preparing URL for parsing\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    page = requests.get (URL, headers = headers)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    #Collecting title and genre with bs4\n",
    "    g = soup.find(class_ = 'breadcrumb').get_text()\n",
    "    g_list = g.split(\"\\n\")\n",
    "\n",
    "    #Selecting title and genre from breadcrump list\n",
    "    title = g_list[-2]\n",
    "    genre = g_list[-4]\n",
    "\n",
    "\n",
    "    #Collecting price\n",
    "    price = soup.find(class_ = 'price_color').get_text()\n",
    "\n",
    "\n",
    "    #Collecting quantity and availability (in_stock)\n",
    "    availability = soup.find(class_ = 'instock availability').get_text()\n",
    "\n",
    "    #Selecting quantity by selecting only numeric values\n",
    "    quantity = re.findall(r'\\d+', availability)\n",
    "    quantity = quantity.pop()\n",
    "\n",
    "    #Using if/else statement to determine availability\n",
    "    quantity = int(quantity)\n",
    "    if quantity <= 0:\n",
    "        in_stock = 'no'\n",
    "    else:\n",
    "        in_stock = 'yes'\n",
    "\n",
    "\n",
    "    #Collecting UPC and product type from informational table    \n",
    "    t_info = soup.find(class_ = 'table table-striped').get_text()\n",
    "    t_info = t_info.split()\n",
    "\n",
    "    #Selecting UPC and product type\n",
    "    UPC = t_info[0]\n",
    "    UPC = UPC[2:]\n",
    "    product_type = t_info[2]\n",
    "    product_type = product_type[4:]\n",
    "\n",
    "\n",
    "    #Determining when the data was accessed\n",
    "    date_accessed = dt.datetime.now()\n",
    "    date_accessed = date_accessed.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    #Display results\n",
    "    results = [title, genre, price, quantity, in_stock, UPC, product_type, date_accessed]\n",
    "    print(results)\n",
    "    with open('ProductInformation.csv', 'a+', newline = '', encoding = 'UTF8') as a:\n",
    "        writer = csv.writer(a)\n",
    "        writer.writerow(results)\n",
    "        \n",
    "productExtract('https://books.toscrape.com/catalogue/in-her-wake_980/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for collecting the URL of every product in a page\n",
    "#Part 2 of the string\n",
    "\n",
    "#Preparing CSV that links will be added to\n",
    "header = ['title','link', 'date_accessed']\n",
    "with open('ProductURL.csv', 'w', newline = '', encoding = 'UTF8') as b:\n",
    "    writer = csv.writer(b)\n",
    "    writer.writerow(header)\n",
    "\n",
    "def extract(URL):\n",
    "    \n",
    "    #Preparing url for parsing   \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    page = requests.get (URL, headers = headers)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "       \n",
    "    #Collecting all links on URL on a list\n",
    "    links = soup.find_all('a')\n",
    "    links = list(links)\n",
    "\n",
    "    #Categories of urls collected\n",
    "    book_url = []\n",
    "    book_url_2 = []\n",
    "    category_url = []\n",
    "    page_url = []\n",
    "    index_url = []\n",
    "\n",
    "    #Categorizing links and sorting them into lists\n",
    "    for x in links:\n",
    "        #Extract links from list and turning them into strings\n",
    "        href = x[\"href\"]\n",
    "        href = str(href)\n",
    "        \n",
    "        #Filter out non-book links\n",
    "        if 'category/books' in href: \n",
    "            category_url.append(href)\n",
    "        \n",
    "        elif '../index.html' in href:\n",
    "            index_url.append(href)\n",
    "            \n",
    "        #Filter out non-book links\n",
    "        elif 'page-' in href:\n",
    "            nextPage = href\n",
    "\n",
    "        #Adding book links to a list\n",
    "        elif 'index.html' not in href[0:10]:\n",
    "            book_url_2.append(href)\n",
    "\n",
    "    #Collecting unique links for books, each book has two links on each page    \n",
    "    for hrefLink in book_url_2:\n",
    "        if hrefLink not in book_url:\n",
    "            book_url.append(hrefLink)\n",
    "            \n",
    "            end = hrefLink.index('_')\n",
    "            title = hrefLink[:end]\n",
    "            title = title.replace('-', ' ')\n",
    "            title = title.title()\n",
    "\n",
    "            date_accessed = dt.datetime.now()\n",
    "            date_accessed = date_accessed.strftime(\"%Y-%m-%d %H:%M\")\n",
    "            \n",
    "            row = (title, hrefLink, date_accessed)\n",
    "            with open('ProductURL.csv', 'a+', newline = '\\n', encoding = 'UTF8') as b:\n",
    "                writer = csv.writer(b)\n",
    "                writer.writerow(row)              \n",
    "    \n",
    "extract('https://books.toscrape.com/catalogue/page-1.html')\n",
    "\n",
    "\n",
    "print('your file is ready mr. tanabe ❤️ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09461507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for moving pages\n",
    "#Part 3 of the string\n",
    "\n",
    "\n",
    "def extract(URL):\n",
    "    while True:\n",
    "        currentPage = URL[42:-5]\n",
    "        currentPage = int(currentPage)\n",
    "        print('Currently parsing Page ', currentPage)\n",
    "\n",
    "        #Preparing url for parsing   \n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "        page = requests.get (URL, headers = headers)\n",
    "        soup = BS(page.content, 'html.parser')\n",
    "\n",
    "        #Collecting all links on URL on a list\n",
    "        links = soup.find_all('a')\n",
    "        links = list(links)\n",
    "\n",
    "        #Categories of urls collected\n",
    "        page_url = []\n",
    "\n",
    "        #Categorizing links and sorting them into lists\n",
    "        for x in links:\n",
    "        #Extract links from list and turning them into strings\n",
    "            href = x[\"href\"]\n",
    "            href = str(href)\n",
    "\n",
    "        #Categorizing links and sorting them into lists\n",
    "        for x in links:\n",
    "            #Find the link to the next page\n",
    "            if 'page-' in href:\n",
    "                nextPage = href #NOTE There are four links that direct to a different page on the site. The last one always directs to the next page. It will get rewritten'\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        nextPage = nextPage[5:-5]\n",
    "        URL = 'https://books.toscrape.com/catalogue/page-' + nextPage + '.html'\n",
    "        print('the next page is ' + nextPage)\n",
    "        if currentPage == 10:\n",
    "            break\n",
    "    return currentPage\n",
    "\n",
    "extract('https://books.toscrape.com/catalogue/page-1.html')\n",
    "\n",
    "\n",
    "print('your file is ready mr. tanabe ❤️ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de518eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First version of the file put together with comments from Mr. Alialy\n",
    "\n",
    "#Preparing CSV that links will be added to\n",
    "header = ['title','link', 'date_accessed']\n",
    "with open('ProductURL.csv', 'w', newline = '', encoding = 'UTF8') as b:\n",
    "    writer = csv.writer(b)\n",
    "    writer.writerow(header)\n",
    "\n",
    "#Preparing CSV that information will be added to\n",
    "productHeader = ['title', 'genre', 'price', 'quantity', 'in_stock', 'UPC', 'product_type', 'date_accessed']\n",
    "with open('ProductInformation.csv', 'w', newline = '', encoding = 'UTF8') as a:\n",
    "    writer = csv.writer(a)\n",
    "    writer.writerow(productHeader)\n",
    "    \n",
    "def productExtract(URL):\n",
    "    #Preparing URL for parsing\n",
    "    URL = 'https://books.toscrape.com/catalogue/' + URL\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    page = requests.get (URL, headers = headers)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "    #Collecting title and genre with bs4\n",
    "    g = soup.find(class_ = 'breadcrumb').get_text()\n",
    "    g_list = g.split(\"\\n\")\n",
    "\n",
    "    #Selecting title and genre from breadcrump list\n",
    "    title = g_list[-2]\n",
    "    genre = g_list[-4]\n",
    "#kyle - remove whitespace from g list. ALSO pull when to stop page from the actual page itself\n",
    "\n",
    "\n",
    "    #Collecting price\n",
    "    price = soup.find(class_ = 'price_color').get_text()\n",
    "\n",
    "\n",
    "    #Collecting quantity and availability (in_stock)\n",
    "    availability = soup.find(class_ = 'instock availability').get_text()\n",
    "\n",
    "    #Selecting quantity by selecting only numeric values\n",
    "    quantity = re.findall(r'\\d+', availability)\n",
    "    quantity = quantity.pop()\n",
    "\n",
    "    #Using if/else statement to determine availability\n",
    "    quantity = int(quantity)\n",
    "    if quantity <= 0:\n",
    "        in_stock = 'no'\n",
    "    else:\n",
    "        in_stock = 'yes'\n",
    "\n",
    "\n",
    "    #Collecting UPC and product type from informational table    \n",
    "    t_info = soup.find(class_ = 'table table-striped').get_text()\n",
    "    t_info = t_info.split()\n",
    "\n",
    "    #Selecting UPC and product type\n",
    "    UPC = t_info[0]\n",
    "    UPC = UPC[2:]\n",
    "    product_type = t_info[2]\n",
    "    product_type = product_type[4:]\n",
    "    #MAKE THIS MORE EXPLICIT, Pull table then pull based on tags\n",
    "\n",
    "\n",
    "    #Determining when the data was accessed\n",
    "    date_accessed = dt.datetime.now()\n",
    "    date_accessed = date_accessed.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    #Display results\n",
    "    results = [title, genre, price, quantity, in_stock, UPC, product_type, date_accessed]\n",
    "    with open('ProductInformation.csv', 'a+', newline = '', encoding = 'UTF8') as a:\n",
    "        writer = csv.writer(a)\n",
    "        writer.writerow(results)\n",
    "    \n",
    "def extract(URL):\n",
    "#add a max index through a for loop\n",
    "#NEVER USE THE SAME THING\n",
    "    while True:\n",
    "        currentPage = URL[42:-5]\n",
    "        currentPage = int(currentPage)\n",
    "        print(currentPage)\n",
    "        #currentPage = int(currentPage)\n",
    "        print('Currently parsing Page ', currentPage)\n",
    "\n",
    "        #Preparing url for parsing   \n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "        page = requests.get (URL, headers = headers)\n",
    "        soup = BS(page.content, 'html.parser')\n",
    "\n",
    "        #Collecting all links on URL on a list\n",
    "        links = soup.find_all('a')\n",
    "        links = list(links)\n",
    "\n",
    "        #Categories of urls collected\n",
    "        book_url = []\n",
    "        book_url_2 = []\n",
    "        category_url = []\n",
    "        page_url = []\n",
    "        index_url = []\n",
    "\n",
    "        #Categorizing links and sorting them into lists\n",
    "        #REWRITE ALL DIS SHIT, USE CLASS TO ONLY SELECT ITEMS\n",
    "        for x in links:\n",
    "        #Extract links from list and turning them into strings\n",
    "            href = x[\"href\"]\n",
    "            href = str(href)\n",
    "\n",
    "            #Find the link to the next page\n",
    "            if 'page-' in href:\n",
    "                nextPage = href #NOTE There are four links that direct to a different page on the site. The last one always directs to the next page. It will get rewritten'\n",
    "\n",
    "            #Create list of categories\n",
    "            elif 'category/books' in href: \n",
    "                category_url.append(href)\n",
    "\n",
    "            #Put indices in one list\n",
    "            elif '../index.html' in href:\n",
    "                index_url.append(href)\n",
    "\n",
    "            #Adding book links to a list\n",
    "            elif 'index.html' not in href[0:10]:\n",
    "                book_url_2.append(href)\n",
    "\n",
    "        #Collecting unique links for books, each book has two links on each page    \n",
    "        for hrefLink in book_url_2:\n",
    "            if hrefLink not in book_url:\n",
    "                productExtract(hrefLink)\n",
    "                book_url.append(hrefLink)\n",
    "\n",
    "                end = hrefLink.index('_')\n",
    "                title = hrefLink[:end]\n",
    "                title = title.replace('-', ' ')\n",
    "                title = title.title()\n",
    "\n",
    "                date_accessed = dt.datetime.now()\n",
    "                date_accessed = date_accessed.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "                row = (title, hrefLink, date_accessed)\n",
    "                with open('ProductURL.csv', 'a+', newline = '\\n', encoding = 'UTF8') as b:\n",
    "                    writer = csv.writer(b)\n",
    "                    writer.writerow(row)\n",
    "                    \n",
    "\n",
    "        nextPage = currentPage + 1\n",
    "        nextPage2 = str(nextPage)\n",
    "        URL = 'https://books.toscrape.com/catalogue/page-' + nextPage2 + '.html'\n",
    "        \n",
    "        #do if theres no next, then stop\n",
    "        if nextPage == 51:\n",
    "            print('Parsing successful! Please check your new CSV files.')\n",
    "            break\n",
    "        else: \n",
    "            print('The next page is: ')\n",
    "            continue       \n",
    "    \n",
    "page2 = 'https://books.toscrape.com/catalogue/page-1.html'\n",
    "extract(page2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ef068d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Version two of part 1\n",
    "URL = 'https://books.toscrape.com/catalogue/frankenstein_20/index.html'\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "def productExtract(url):\n",
    "    page = requests.get(URL, headers = headers)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "\n",
    "    #Collect the title\n",
    "    title = soup.html.title.text\n",
    "    title = title[:title.index('|')].strip()\n",
    "\n",
    "    #Collect the genre, which is located in a breadcrumb list\n",
    "    genre = soup.find('ul').text\n",
    "    #Create list of breadcrumb items\n",
    "    genre = genre.split('\\n')\n",
    "    genre = list(filter(None, genre))[-2]\n",
    "\n",
    "    #Collecting price\n",
    "    price = soup.find(class_ = 'price_color').get_text()\n",
    "\n",
    "\n",
    "    #Collecting quantity and availability (in_stock)\n",
    "    #Returns a statement ie: 'In stock (21) available'\n",
    "    availability = soup.find(class_ = 'instock availability').get_text()\n",
    "\n",
    "    #Selecting quantity by filtering numeric values into a list\n",
    "    #This puts numeric values into a list\n",
    "    quantity = re.findall(r'\\d+', availability)\n",
    "    #Removes the quantity + changes to integer type\n",
    "    quantity = int(quantity.pop())\n",
    "\n",
    "\n",
    "    #Using if/else statement to determine availability status\n",
    "    quantity = int(quantity)\n",
    "    if quantity <= 0:\n",
    "        in_stock = 'no'\n",
    "    else:\n",
    "        in_stock = 'yes'\n",
    "\n",
    "    #Selecting UPC and product type\n",
    "    UPC = soup.html.table.td.text\n",
    "    product_type = list(soup.find_all('td'))[1].text\n",
    "\n",
    "    #Determining when the data was accessed\n",
    "    date_accessed = dt.datetime.now()\n",
    "    date_accessed = date_accessed.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    #Display results\n",
    "    results = [title, genre, price, quantity, in_stock, UPC, product_type, date_accessed]\n",
    "\n",
    "    with open('ProductInformation.csv', 'a+', newline = '', encoding = 'UTF8') as a:\n",
    "        writer = csv.writer(a)\n",
    "        writer.writerow(results)\n",
    "        \n",
    "productExtract(URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f1a98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2 of parts 2 and 3 put together\n",
    "URL = 'https://books.toscrape.com/catalogue/page-1.html'\n",
    "page = requests.get (URL)\n",
    "soup = BS(page.content, 'html.parser')\n",
    "\n",
    "#Preparing CSV that links will be added to\n",
    "header = ['title','link', 'dateAccessed']\n",
    "with open('ProductURL.csv', 'w', newline = '', encoding = 'UTF8') as b:\n",
    "    writer = csv.writer(b)\n",
    "    writer.writerow(header) \n",
    "   \n",
    "#Pass URL through this function to collect all links in the function\n",
    "def pageCollect(url):\n",
    "    page = requests.get (URL)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "    \n",
    "    bookContainers = soup.find_all(\"li\", {'class':'col-xs-6 col-sm-4 col-md-3 col-lg-3'})\n",
    "\n",
    "    for container in bookContainers:\n",
    "        containerTitle = container.find('a').find('img').get('alt')\n",
    "        \n",
    "        href = container.find('a').get('href')\n",
    "        bookLink = 'https://books.toscrape.com/catalogue/'\n",
    "\n",
    "        dateAccessed = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "        row = (containerTitle, bookLink, dateAccessed)\n",
    "        with open('ProductURL.csv', 'a+', newline = '\\n', encoding = 'UTF8') as b:\n",
    "            writer = csv.writer(b)\n",
    "            writer.writerow(row)\n",
    "        \n",
    "#For finding the total number of pages in the online store\n",
    "pageNum = soup.find(\"ul\", {\"class\": \"pager\"}).text.strip()\n",
    "pageTotal = int(pageNum[pageNum.index('of') + 3 : pageNum.index('next')])\n",
    "pageCurrent = 0\n",
    "\n",
    "#For moving between pages\n",
    "#GENERATES A NEW URL\n",
    "# for i in range(1, pageTotal + 1):\n",
    "for i in range(1, 5):\n",
    "    URL = 'https://books.toscrape.com/catalogue/page-' + str(i) + '.html'\n",
    "    pageCollect(URL)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fc8491a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently parsing through page 1.\n",
      "Currently parsing through page 2.\n",
      "Currently parsing through page 3.\n",
      "Currently parsing through page 4.\n",
      "Currently parsing through page 5.\n",
      "Currently parsing through page 6.\n",
      "Currently parsing through page 7.\n",
      "Currently parsing through page 8.\n",
      "Currently parsing through page 9.\n",
      "Currently parsing through page 10.\n",
      "Currently parsing through page 11.\n",
      "Currently parsing through page 12.\n",
      "Currently parsing through page 13.\n",
      "Currently parsing through page 14.\n",
      "Currently parsing through page 15.\n",
      "Currently parsing through page 16.\n",
      "Currently parsing through page 17.\n",
      "Currently parsing through page 18.\n",
      "Currently parsing through page 19.\n",
      "Currently parsing through page 20.\n",
      "Currently parsing through page 21.\n",
      "Currently parsing through page 22.\n",
      "Currently parsing through page 23.\n",
      "Currently parsing through page 24.\n",
      "Currently parsing through page 25.\n",
      "Currently parsing through page 26.\n",
      "Currently parsing through page 27.\n",
      "Currently parsing through page 28.\n",
      "Currently parsing through page 29.\n",
      "Currently parsing through page 30.\n",
      "Currently parsing through page 31.\n",
      "Currently parsing through page 32.\n",
      "Currently parsing through page 33.\n",
      "Currently parsing through page 34.\n",
      "Currently parsing through page 35.\n",
      "Currently parsing through page 36.\n",
      "Currently parsing through page 37.\n",
      "Currently parsing through page 38.\n",
      "Currently parsing through page 39.\n",
      "Currently parsing through page 40.\n",
      "Currently parsing through page 41.\n",
      "Currently parsing through page 42.\n",
      "Currently parsing through page 43.\n",
      "Currently parsing through page 44.\n",
      "Currently parsing through page 45.\n",
      "Currently parsing through page 46.\n",
      "Currently parsing through page 47.\n",
      "Currently parsing through page 48.\n",
      "Currently parsing through page 49.\n",
      "Currently parsing through page 50.\n"
     ]
    }
   ],
   "source": [
    "#Final string: version 2 of the whole script\n",
    "\n",
    "#Creating CSV that links collected from each store page will be added to\n",
    "header = ['title','link', 'dateAccessed']\n",
    "with open('ProductURL.csv', 'w', newline = '', encoding = 'UTF8') as b:\n",
    "    writer = csv.writer(b)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "#Creating CSV that information from each product will be added to\n",
    "productHeader = ['title', 'genre', 'price', 'quantity', 'inStock', 'UPC', 'productType', 'dateAccessed']\n",
    "with open('ProductInformation.csv', 'w', newline = '', encoding = 'UTF8') as a:\n",
    "    writer = csv.writer(a)\n",
    "    writer.writerow(productHeader)\n",
    "\n",
    "#Function for extracting product information\n",
    "def productExtract(url):\n",
    "    page = requests.get(url, headers = headers)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "\n",
    "    #Select the title\n",
    "    title = soup.html.title.text\n",
    "    title = title[:title.index('|')].strip()\n",
    "\n",
    "    #Select the genre, which is only located in a breadcrumb list\n",
    "    genre = soup.find('ul').text\n",
    "    #Create list of breadcrumb items\n",
    "    genre = genre.split('\\n')\n",
    "    genre = list(filter(None, genre))[-2]\n",
    "\n",
    "    #Selecting price\n",
    "    price = soup.find(class_ = 'price_color').get_text()\n",
    "\n",
    "    #Selecting quantity and availability (inStock)\n",
    "    #Returns a statement ie: 'In stock (21) available'\n",
    "    availability = soup.find(class_ = 'instock availability').text\n",
    "\n",
    "    #Selecting quantity by filtering numeric values into a list\n",
    "    quantity = re.findall(r'\\d+', availability)[0]\n",
    "\n",
    "    #Using quantity to determine availability\n",
    "    quantity = int(quantity)\n",
    "    if quantity <= 0:\n",
    "        inStock = 'no'\n",
    "    else:\n",
    "        inStock = 'yes'\n",
    "\n",
    "    #Selecting UPC and product type\n",
    "    UPC = soup.html.table.td.text\n",
    "    productType = list(soup.find_all('td'))[1].text\n",
    "\n",
    "    #Determine when data was accessed\n",
    "    dateAccessed = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    #Write results onto CSV with product information\n",
    "    results = [title, genre, price, quantity, inStock, UPC, productType, dateAccessed]\n",
    "    with open('ProductInformation.csv', 'a+', newline = '', encoding = 'UTF8') as a:\n",
    "        writer = csv.writer(a)\n",
    "        writer.writerow(results)\n",
    "        \n",
    "#Function for collecting URL for every product in store page\n",
    "def pageCollect(url2):\n",
    "    page = requests.get(url2)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "    \n",
    "    #Collect all containers containing product title and link\n",
    "    bookContainers = soup.find_all(\"li\", {'class':'col-xs-6 col-sm-4 col-md-3 col-lg-3'})\n",
    "\n",
    "    for container in bookContainers:\n",
    "        #Select title\n",
    "        containerTitle = container.find('a').find('img').get('alt')\n",
    "        \n",
    "        #Select link\n",
    "        href = container.find('a').get('href')\n",
    "        bookLink = 'https://books.toscrape.com/catalogue/' + href\n",
    "        \n",
    "        #Determine when data was accessed\n",
    "        dateAccessed = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        \n",
    "        #Write results onto CSV with product links\n",
    "        row = (containerTitle, bookLink, dateAccessed)\n",
    "        with open('ProductURL.csv', 'a+', newline = '\\n', encoding = 'UTF8') as b:\n",
    "            writer = csv.writer(b)\n",
    "            writer.writerow(row)\n",
    "        productExtract(bookLink)\n",
    "        \n",
    "#Finds total number of product store pages\n",
    "#Returns string ie: 'Page 1 of x (x = number of pages)'\n",
    "pageNum = soup.find(\"ul\", {\"class\": \"pager\"}).text.strip()\n",
    "pageTotal = int(pageNum[pageNum.index('of') + 3 : pageNum.index('next')])\n",
    "\n",
    "#For moving between pages\n",
    "#GENERATES A NEW URL\n",
    "for i in range(1, pageTotal + 1):\n",
    "    print('Currently parsing through page ' + str(i) + '.')\n",
    "    URL = 'https://books.toscrape.com/catalogue/page-' + str(i) + '.html'\n",
    "    pageCollect(URL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19dbf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
